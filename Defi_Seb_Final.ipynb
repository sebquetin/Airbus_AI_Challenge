{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py    \n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "from pylab import savefig\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from pylab import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ACP\n",
    "import sklearn.decomposition as sd\n",
    "import sklearn.preprocessing as sp\n",
    "\n",
    "# Hierarchical clustering\n",
    "import scipy.cluster.hierarchy as sch\n",
    "# LOF\n",
    "import sklearn.neighbors as sn\n",
    "# Isolation Forest\n",
    "import sklearn.ensemble as se\n",
    "\n",
    "import sklearn.svm as ssvm\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set()\n",
    "sb.set_style(\"whitegrid\")\n",
    "\n",
    "import pywt\n",
    "from pywt import wavedec\n",
    "import scipy.stats as stats\n",
    "from statsmodels.robust import mad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= pd.read_hdf('train.hdf5')\n",
    "v=pd.read_hdf('validation.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=pd.DataFrame(f)\n",
    "v=pd.DataFrame(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_hdf('test.hdf5')\n",
    "test=pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_anom():\n",
    "    data = 0x1323b136220e15dd4d1ff9681fa066ada681fc081ddd4c61e1c0fd118121ee524ca9286fc08bf58c64128c557cc8bc73212e3eec926452fbc7d9d9ace6eb98f9b381c8fbdd5d24743fd59\n",
    "    res = list(map(lambda x: x == '1', bin(data)[2:]))\n",
    "    return [False]*(594-len(res)) + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reponse=pd.read_excel('reponseEmmeline.xlsx')\n",
    "reponse=get_true_anom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(reponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=pd.concat([f,v],axis=0)\n",
    "all_data=all_data.reset_index()\n",
    "all_data=all_data.drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2271, 61441) (2271, 61441)\n",
      "137162577 15870780\n"
     ]
    }
   ],
   "source": [
    "wf = \"haar\"\n",
    "\n",
    "Coeff = []\n",
    "TCoeff = []\n",
    "for i in range(2271) :\n",
    "    #Apply wavelet decomposition\n",
    "    x=all_data.iloc[i]\n",
    "    coeffs = pywt.wavedec(x,wf)\n",
    "    coeffs_flatten = np.hstack(coeffs)\n",
    "    Coeff.append(coeffs_flatten)\n",
    "    \n",
    "    # Compute universal Threshold http://jseabold.net/blog/2012/02/23/wavelet-regression-in-python/\n",
    "    sigma = mad(coeffs[-1])\n",
    "    uthresh = sigma*np.sqrt(2*np.log(61441))\n",
    "    # Apply Threshold on 4 last levels\n",
    "    coeffs_thresh = [pywt.threshold(c, uthresh, mode=\"hard\") if i<=3 else c for i,c in enumerate(coeffs[::-1])]\n",
    "    coeffs_thresh_flatten = np.hstack(coeffs_thresh[::-1])\n",
    "    TCoeff.append(coeffs_thresh_flatten)\n",
    "    \n",
    "Coeff = np.array(Coeff)\n",
    "TCoeff = np.array(TCoeff)\n",
    "print(Coeff.shape, TCoeff.shape)\n",
    "print(np.sum(Coeff!=0), np.sum(TCoeff!=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2271"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Coeff1=Coeff[:1677,:]\n",
    "Coeff2=Coeff[1677:,:]\n",
    "\n",
    "TCoeff1=TCoeff[:1677,:]\n",
    "TCoeff2=TCoeff[1677:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_result(liste,vrai=reponse):\n",
    "    num=0\n",
    "    for i in range(594):\n",
    "        if vrai[i]==liste[i]:\n",
    "            num+=1\n",
    "    return num/594"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def scores(liste,vrai=reponse):\n",
    "    anomalies=[1 if i in liste else 0 for i in range(594)]\n",
    "    precision= len(np.intersect1d(liste,np.where(vrai)))/len(liste)\n",
    "    #print(np.where(anomalies==1))\n",
    "    #print(precision)\n",
    "    recall= len(np.intersect1d(liste,np.where(vrai)))/len(np.where(vrai)[0])\n",
    "    #print(recall)\n",
    "    f1score= 2*((precision*recall)/(precision+recall))\n",
    "    print('f1score :', f1score, ' ; precision : ',precision , ' ; recall : ',recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm as ssvm\n",
    "def detection_anomaly(acp,acp2):\n",
    "    OCS = ssvm.OneClassSVM(kernel=\"rbf\", nu=0.05)\n",
    "\n",
    "    fitter1=OCS.fit(acp)\n",
    "    anoSVM = fitter1.predict(acp2)\n",
    "\n",
    "    for i in range(len(anoSVM)) : \n",
    "        if anoSVM[i]==1 : \n",
    "            anoSVM[i]=0\n",
    "        else :\n",
    "            anoSVM[i]= 1 \n",
    "    print(\"nombre d'anomalies trouvées par SVM\",sum(anoSVM))\n",
    "\n",
    "    contamination=0.5\n",
    "    metric = \"euclidean\"\n",
    "    clf = sn.LocalOutlierFactor(n_neighbors=5, contamination=contamination, metric=metric,novelty=True)\n",
    "    fitter2 = clf.fit(acp)\n",
    "    anoLOF=fitter2.predict(acp2)\n",
    "\n",
    "    for i in range(len(anoLOF)) : \n",
    "        if anoLOF[i]==1 : \n",
    "            anoLOF[i]=0\n",
    "        else :\n",
    "            anoLOF[i]= 1 \n",
    "    \n",
    "    print(\"nombre d'anomalies trouvées par LOF\",sum(anoLOF))\n",
    "\n",
    "    clf = IsolationForest(behaviour='new',contamination=0.5)\n",
    "    fitter3=clf.fit(acp)\n",
    "    anoISO=fitter3.predict(acp2)\n",
    "\n",
    "    for i in range(len(anoISO)) : \n",
    "        if anoISO[i]==1 : \n",
    "            anoISO[i]=0\n",
    "        else :\n",
    "            anoISO[i]= 1 \n",
    "    print(\"nombre d'anomalies trouvées par Isolation Forest\",sum(anoISO))\n",
    "    \n",
    "    listoutliersintersect=np.intersect1d(np.where(anoLOF==1),np.where(anoSVM==1),np.where(anoISO==1))\n",
    "    listoutliersunion=np.union1d(np.union1d(np.where(anoLOF==1),np.where(anoSVM==1)),np.where(anoISO==1))\n",
    "    print('scores Isolation Forest')\n",
    "    scores(np.where(anoISO==1)[0])\n",
    "    print('scores SVM')\n",
    "    scores(np.where(anoSVM==1)[0])\n",
    "    print('scores LOF')\n",
    "    scores(np.where(anoLOF==1)[0])\n",
    "    print('scores union des techniques')\n",
    "    scores(listoutliersunion)\n",
    "    print('scores intersection des techniques')\n",
    "    scores(listoutliersintersect)\n",
    "    listoutliersintersect1=np.intersect1d(np.where(anoLOF==1),np.where(anoSVM==1))\n",
    "    print('scores intersection SVM LOF')\n",
    "    scores(listoutliersintersect1)\n",
    "    listoutliersintersect2=np.intersect1d(np.where(anoLOF==1),np.where(anoISO==1))\n",
    "    print('scores intersection Isolation Forest LOF')\n",
    "    scores(listoutliersintersect2) \n",
    "    listoutliersintersect3=np.intersect1d(np.where(anoISO==1),np.where(anoSVM==1))\n",
    "    print('scores intersection SVM Isolation Forest')\n",
    "    scores(listoutliersintersect3)\n",
    "    listoutliersunion1=np.union1d(np.where(anoLOF==1),np.where(anoSVM==1))\n",
    "    print('scores union SVM LOF')\n",
    "    scores(listoutliersunion1)\n",
    "    listoutliersunion2=np.union1d(np.where(anoLOF==1),np.where(anoISO==1))\n",
    "    print('scores union Isolation Forest LOF')\n",
    "    scores(listoutliersunion2) \n",
    "    listoutliersunion3=np.union1d(np.where(anoISO==1),np.where(anoSVM==1))\n",
    "    print('scores union SVM Isolation Forest')\n",
    "    scores(listoutliersunion3)\n",
    "    return anoSVM,anoLOF,anoISO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/insa/anaconda/envs/GPU/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'anomalies trouvées par SVM 261\n",
      "nombre d'anomalies trouvées par LOF 321\n",
      "nombre d'anomalies trouvées par Isolation Forest 292\n",
      "scores Isolation Forest\n",
      "f1score : 0.5840407470288624  ; precision :  0.589041095890411  ; recall :  0.5791245791245792\n",
      "scores SVM\n",
      "f1score : 0.9247311827956989  ; precision :  0.9885057471264368  ; recall :  0.8686868686868687\n",
      "scores LOF\n",
      "f1score : 0.7411003236245954  ; precision :  0.7133956386292835  ; recall :  0.7710437710437711\n",
      "scores union des techniques\n",
      "f1score : 0.7337110481586401  ; precision :  0.6332518337408313  ; recall :  0.8720538720538721\n",
      "scores intersection des techniques\n",
      "f1score : 0.870722433460076  ; precision :  1.0  ; recall :  0.7710437710437711\n",
      "scores intersection SVM LOF\n",
      "f1score : 0.870722433460076  ; precision :  1.0  ; recall :  0.7710437710437711\n",
      "scores intersection Isolation Forest LOF\n",
      "f1score : 0.5697211155378485  ; precision :  0.697560975609756  ; recall :  0.48148148148148145\n",
      "scores intersection SVM Isolation Forest\n",
      "f1score : 0.7261146496815287  ; precision :  0.9827586206896551  ; recall :  0.5757575757575758\n",
      "scores union SVM LOF\n",
      "f1score : 0.7938461538461539  ; precision :  0.7308781869688386  ; recall :  0.8686868686868687\n",
      "scores union Isolation Forest LOF\n",
      "f1score : 0.7319148936170213  ; precision :  0.6323529411764706  ; recall :  0.8686868686868687\n",
      "scores union SVM Isolation Forest\n",
      "f1score : 0.7662721893491125  ; precision :  0.683377308707124  ; recall :  0.8720538720538721\n"
     ]
    }
   ],
   "source": [
    "anoSVM,anoLOF,anoISO=detection_anomaly(Coeff[:1677,:1024],Coeff[1677:,:1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9292929292929293"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result(anoSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_smooth=all_data.ewm(alpha=0.3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n",
      "2190\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "2201\n",
      "2202\n",
      "2203\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2242\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n",
      "2268\n",
      "2269\n",
      "2270\n",
      "(2271, 61441) (2271, 61441)\n",
      "139528424 14406589\n"
     ]
    }
   ],
   "source": [
    "wf = \"haar\"\n",
    "\n",
    "Coeff_smooth = []\n",
    "TCoeff_smooth = []\n",
    "for i in range(2271) :\n",
    "    print(i)\n",
    "    #Apply wavelet decomposition\n",
    "    x=all_data_smooth.iloc[i]\n",
    "    coeffs = pywt.wavedec(x,wf)\n",
    "    coeffs_flatten = np.hstack(coeffs)\n",
    "    Coeff_smooth.append(coeffs_flatten)\n",
    "    \n",
    "    # Compute universal Threshold http://jseabold.net/blog/2012/02/23/wavelet-regression-in-python/\n",
    "    sigma = mad(coeffs[-1])\n",
    "    uthresh = sigma*np.sqrt(2*np.log(61441))\n",
    "    # Apply Threshold on 4 last levels\n",
    "    coeffs_thresh = [pywt.threshold(c, uthresh, mode=\"hard\") if i<=3 else c for i,c in enumerate(coeffs[::-1])]\n",
    "    coeffs_thresh_flatten = np.hstack(coeffs_thresh[::-1])\n",
    "    TCoeff_smooth.append(coeffs_thresh_flatten)\n",
    "    \n",
    "Coeff_smooth = np.array(Coeff_smooth)\n",
    "TCoeff_smooth = np.array(TCoeff_smooth)\n",
    "print(Coeff_smooth.shape, TCoeff_smooth.shape)\n",
    "print(np.sum(Coeff_smooth!=0), np.sum(TCoeff_smooth!=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/insa/anaconda/envs/GPU/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'anomalies trouvées par SVM 125\n",
      "nombre d'anomalies trouvées par LOF 565\n",
      "nombre d'anomalies trouvées par Isolation Forest 364\n",
      "scores Isolation Forest\n",
      "f1score : 0.6293494704992435  ; precision :  0.5714285714285714  ; recall :  0.7003367003367004\n",
      "scores SVM\n",
      "f1score : 0.4312796208530805  ; precision :  0.728  ; recall :  0.3063973063973064\n",
      "scores LOF\n",
      "f1score : 0.6566125290023203  ; precision :  0.5008849557522124  ; recall :  0.9528619528619529\n",
      "scores union des techniques\n",
      "f1score : 0.6528258362168397  ; precision :  0.4964912280701754  ; recall :  0.9528619528619529\n",
      "scores intersection des techniques\n",
      "f1score : 0.4312796208530805  ; precision :  0.728  ; recall :  0.3063973063973064\n",
      "scores intersection SVM LOF\n",
      "f1score : 0.4312796208530805  ; precision :  0.728  ; recall :  0.3063973063973064\n",
      "scores intersection Isolation Forest LOF\n",
      "f1score : 0.6341463414634146  ; precision :  0.5793871866295265  ; recall :  0.7003367003367004\n",
      "scores intersection SVM Isolation Forest\n",
      "f1score : 0.4312796208530805  ; precision :  0.728  ; recall :  0.3063973063973064\n",
      "scores union SVM LOF\n",
      "f1score : 0.6566125290023203  ; precision :  0.5008849557522124  ; recall :  0.9528619528619529\n",
      "scores union Isolation Forest LOF\n",
      "f1score : 0.6528258362168397  ; precision :  0.4964912280701754  ; recall :  0.9528619528619529\n",
      "scores union SVM Isolation Forest\n",
      "f1score : 0.6293494704992435  ; precision :  0.5714285714285714  ; recall :  0.7003367003367004\n"
     ]
    }
   ],
   "source": [
    "anoSVM,anoLOF,anoISO=detection_anomaly(Coeff_smooth[:1677,512:2048],Coeff_smooth[1677:,512:2048])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "anoSVM,anoLOF,anoISO=detection_anomaly(fftCoeff[:1677,:],fftCoeff[1677:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engeneering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features= np.zeros((2271,21))\n",
    "for i in range(2271) : \n",
    "    features[i,0] = np.mean(Coeff[i,:1024])\n",
    "    features[i,1] = np.var(Coeff[i,:1024])\n",
    "    features[i,2] = np.median(Coeff[i,:1024])\n",
    "    features[i,3] = np.real(stats.kurtosis(Coeff[i,:1024],bias=False))\n",
    "    features[i,4] = np.real(stats.skew(Coeff[i,:1024],bias=False))\n",
    "    features[i,5] = np.min(Coeff[i,:1024])\n",
    "    features[i,6] = np.max(Coeff[i,:1024])   \n",
    "    features[i,7] = np.mean(TCoeff[i,:1024])\n",
    "    features[i,8] = np.var(TCoeff[i,:1024])\n",
    "    features[i,9] = np.median(TCoeff[i,:1024])\n",
    "    features[i,10] = np.real(stats.kurtosis(TCoeff[i,:1024],bias=False))\n",
    "    features[i,11] = np.real(stats.skew(TCoeff[i,:1024],bias=False))\n",
    "    features[i,12] = np.min(TCoeff[i,:1024])\n",
    "    features[i,13] = np.max(TCoeff[i,:1024]) \n",
    "    features[i,14] = np.min(all_data.iloc[i])\n",
    "    features[i,15] = np.max(all_data.iloc[i])    \n",
    "    features[i,16] = np.mean(all_data.iloc[i])\n",
    "    features[i,17] = np.var(all_data.iloc[i])\n",
    "    features[i,18] = np.median(all_data.iloc[i])\n",
    "    features[i,19] = np.real(stats.kurtosis(all_data.iloc[i],bias=False))\n",
    "    features[i,20] = np.real(stats.skew(all_data.iloc[i],bias=False))\n",
    "    \n",
    "    #features[i,10]=np.mean(fftCoeff[i])\n",
    "    #features[i,11] = np.var(fftCoeff[i])\n",
    "    #features[i,12] = np.median(fftCoeff[i])\n",
    "    #features[i,13] = np.real(stats.kurtosis(fftCoeff[i],bias=False))\n",
    "    #features[i,14] = np.real(stats.skew(fftCoeff[i],bias=False))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/insa/anaconda/envs/GPU/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'anomalies trouvées par SVM 281\n",
      "nombre d'anomalies trouvées par LOF 442\n",
      "nombre d'anomalies trouvées par Isolation Forest 392\n",
      "scores Isolation Forest\n",
      "f1score : 0.8592162554426704  ; precision :  0.7551020408163265  ; recall :  0.9966329966329966\n",
      "scores SVM\n",
      "f1score : 0.9619377162629758  ; precision :  0.9893238434163701  ; recall :  0.936026936026936\n",
      "scores LOF\n",
      "f1score : 0.7794316644113668  ; precision :  0.6515837104072398  ; recall :  0.9696969696969697\n",
      "scores union des techniques\n",
      "f1score : 0.7754569190600521  ; precision :  0.6332622601279317  ; recall :  1.0\n",
      "scores intersection des techniques\n",
      "f1score : 0.9618055555555556  ; precision :  0.992831541218638  ; recall :  0.9326599326599326\n",
      "scores intersection SVM LOF\n",
      "f1score : 0.9618055555555556  ; precision :  0.992831541218638  ; recall :  0.9326599326599326\n",
      "scores intersection Isolation Forest LOF\n",
      "f1score : 0.8670694864048338  ; precision :  0.7863013698630137  ; recall :  0.9663299663299664\n",
      "scores intersection SVM Isolation Forest\n",
      "f1score : 0.9619377162629758  ; precision :  0.9893238434163701  ; recall :  0.936026936026936\n",
      "scores union SVM LOF\n",
      "f1score : 0.7800269905533064  ; precision :  0.6509009009009009  ; recall :  0.9730639730639731\n",
      "scores union Isolation Forest LOF\n",
      "f1score : 0.7754569190600521  ; precision :  0.6332622601279317  ; recall :  1.0\n",
      "scores union SVM Isolation Forest\n",
      "f1score : 0.8592162554426704  ; precision :  0.7551020408163265  ; recall :  0.9966329966329966\n"
     ]
    }
   ],
   "source": [
    "anoSVM_features_all_data,anoLOF,anoISO=detection_anomaly(features[:1677,[14,15,18]],features[1677:,[14,15,18]])\n",
    "#meilleure pour le moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/insa/anaconda/envs/GPU/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'anomalies trouvées par SVM 310\n",
      "nombre d'anomalies trouvées par LOF 474\n",
      "nombre d'anomalies trouvées par Isolation Forest 390\n",
      "scores Isolation Forest\n",
      "f1score : 0.8034934497816595  ; precision :  0.7076923076923077  ; recall :  0.9292929292929293\n",
      "scores SVM\n",
      "f1score : 0.8830313014827017  ; precision :  0.864516129032258  ; recall :  0.9023569023569024\n",
      "scores LOF\n",
      "f1score : 0.7341115434500649  ; precision :  0.5970464135021097  ; recall :  0.9528619528619529\n",
      "scores union des techniques\n",
      "f1score : 0.7183271832718326  ; precision :  0.5658914728682171  ; recall :  0.9831649831649831\n",
      "scores intersection des techniques\n",
      "f1score : 0.8762541806020068  ; precision :  0.8704318936877077  ; recall :  0.8821548821548821\n",
      "scores intersection SVM LOF\n",
      "f1score : 0.8762541806020068  ; precision :  0.8704318936877077  ; recall :  0.8821548821548821\n",
      "scores intersection Isolation Forest LOF\n",
      "f1score : 0.8297213622291022  ; precision :  0.7679083094555874  ; recall :  0.9023569023569024\n",
      "scores intersection SVM Isolation Forest\n",
      "f1score : 0.89  ; precision :  0.8811881188118812  ; recall :  0.898989898989899\n",
      "scores union SVM LOF\n",
      "f1score : 0.741025641025641  ; precision :  0.598343685300207  ; recall :  0.9730639730639731\n",
      "scores union Isolation Forest LOF\n",
      "f1score : 0.7167487684729064  ; precision :  0.5650485436893203  ; recall :  0.9797979797979798\n",
      "scores union SVM Isolation Forest\n",
      "f1score : 0.7982708933717578  ; precision :  0.6977329974811083  ; recall :  0.9326599326599326\n"
     ]
    }
   ],
   "source": [
    "anoSVM,anoLOF,anoISO=detection_anomaly(features[:1677,[9,12,13]],features[1677:,[9,12,13]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_centered=all_data.apply(lambda row: (row-np.mean(row))/np.var(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2271, 61441) (2271, 61441)\n",
      "139530240 15582227\n"
     ]
    }
   ],
   "source": [
    "wf = \"haar\"\n",
    "\n",
    "Coeff_centered = []\n",
    "TCoeff_centered = []\n",
    "for i in range(2271) :\n",
    "    #Apply wavelet decomposition\n",
    "    x=all_data_centered.iloc[i]\n",
    "    coeffs = pywt.wavedec(x,wf)\n",
    "    coeffs_flatten = np.hstack(coeffs)\n",
    "    Coeff_centered.append(coeffs_flatten)\n",
    "    \n",
    "    # Compute universal Threshold http://jseabold.net/blog/2012/02/23/wavelet-regression-in-python/\n",
    "    sigma = mad(coeffs[-1])\n",
    "    uthresh = sigma*np.sqrt(2*np.log(61441))\n",
    "    # Apply Threshold on 4 last levels\n",
    "    coeffs_thresh = [pywt.threshold(c, uthresh, mode=\"hard\") if i<=3 else c for i,c in enumerate(coeffs[::-1])]\n",
    "    coeffs_thresh_flatten = np.hstack(coeffs_thresh[::-1])\n",
    "    TCoeff_centered.append(coeffs_thresh_flatten)\n",
    "    \n",
    "Coeff_centered  = np.array(Coeff_centered )\n",
    "TCoeff_centered  = np.array(TCoeff_centered )\n",
    "print(Coeff_centered .shape, TCoeff_centered .shape)\n",
    "print(np.sum(Coeff_centered !=0), np.sum(TCoeff_centered !=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/insa/anaconda/envs/GPU/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'anomalies trouvées par SVM 262\n",
      "nombre d'anomalies trouvées par LOF 320\n",
      "nombre d'anomalies trouvées par Isolation Forest 341\n",
      "scores Isolation Forest\n",
      "f1score : 0.7021943573667712  ; precision :  0.656891495601173  ; recall :  0.7542087542087542\n",
      "scores SVM\n",
      "f1score : 0.9373881932021467  ; precision :  1.0  ; recall :  0.8821548821548821\n",
      "scores LOF\n",
      "f1score : 0.7423014586709887  ; precision :  0.715625  ; recall :  0.7710437710437711\n",
      "scores union des techniques\n",
      "f1score : 0.7460992907801418  ; precision :  0.6446078431372549  ; recall :  0.8855218855218855\n",
      "scores intersection des techniques\n",
      "f1score : 0.8685714285714284  ; precision :  1.0  ; recall :  0.7676767676767676\n",
      "scores intersection SVM LOF\n",
      "f1score : 0.8685714285714284  ; precision :  1.0  ; recall :  0.7676767676767676\n",
      "scores intersection Isolation Forest LOF\n",
      "f1score : 0.6980108499095842  ; precision :  0.75390625  ; recall :  0.6498316498316499\n",
      "scores intersection SVM Isolation Forest\n",
      "f1score : 0.8576923076923078  ; precision :  1.0  ; recall :  0.7508417508417509\n",
      "scores union SVM LOF\n",
      "f1score : 0.8079877112135176  ; precision :  0.7429378531073446  ; recall :  0.8855218855218855\n",
      "scores union Isolation Forest LOF\n",
      "f1score : 0.7407407407407408  ; precision :  0.6419753086419753  ; recall :  0.8754208754208754\n",
      "scores union SVM Isolation Forest\n",
      "f1score : 0.7769571639586411  ; precision :  0.6921052631578948  ; recall :  0.8855218855218855\n"
     ]
    }
   ],
   "source": [
    "anoSVM_coeff_centered,anoLOF,anoISO=detection_anomaly(Coeff_centered [:1677,:1024],Coeff_centered [1677:,:1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_centered= np.zeros((2271,21))\n",
    "for i in range(2271) : \n",
    "    features_centered[i,0] = np.mean(all_data_centered.iloc[i])\n",
    "    features_centered[i,1] = np.var(all_data_centered.iloc[i])\n",
    "    features_centered[i,2] = np.median(all_data_centered.iloc[i])\n",
    "    features_centered[i,3] = np.real(stats.kurtosis(all_data_centered.iloc[i],bias=False))\n",
    "    features_centered[i,4] = np.real(stats.skew(all_data_centered.iloc[i],bias=False))\n",
    "    features_centered[i,5] = np.min(all_data_centered.iloc[i])\n",
    "    features_centered[i,6] = np.max(all_data_centered.iloc[i])\n",
    "    features_centered[i,7] = np.mean(Coeff_centered[i,:1024])\n",
    "    features_centered[i,8] = np.var(Coeff_centered[i,:1024])\n",
    "    features_centered[i,9] = np.median(Coeff_centered[i,:1024])\n",
    "    features_centered[i,10] = np.real(stats.kurtosis(Coeff_centered[i,:1024],bias=False))\n",
    "    features_centered[i,11] = np.real(stats.skew(Coeff_centered[i,:1024],bias=False))\n",
    "    features_centered[i,12] = np.min(Coeff_centered[i,:1024])\n",
    "    features_centered[i,13] = np.max(Coeff_centered[i,:1024])\n",
    "    features_centered[i,14] = np.mean(TCoeff_centered[i,:1024])\n",
    "    features_centered[i,15] = np.var(TCoeff_centered[i,:1024])\n",
    "    features_centered[i,16] = np.median(TCoeff_centered[i,:1024])\n",
    "    features_centered[i,17] = np.real(stats.kurtosis(TCoeff_centered[i,:1024],bias=False))\n",
    "    features_centered[i,18] = np.real(stats.skew(TCoeff_centered[i,:1024],bias=False))\n",
    "    features_centered[i,19] = np.min(TCoeff_centered[i,:1024])\n",
    "    features_centered[i,20] = np.max(TCoeff_centered[i,:1024])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/insa/anaconda/envs/GPU/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'anomalies trouvées par SVM 258\n",
      "nombre d'anomalies trouvées par LOF 444\n",
      "nombre d'anomalies trouvées par Isolation Forest 376\n",
      "scores Isolation Forest\n",
      "f1score : 0.787518573551263  ; precision :  0.7047872340425532  ; recall :  0.8922558922558923\n",
      "scores SVM\n",
      "f1score : 0.9297297297297298  ; precision :  1.0  ; recall :  0.8686868686868687\n",
      "scores LOF\n",
      "f1score : 0.7395411605937922  ; precision :  0.6171171171171171  ; recall :  0.9225589225589226\n",
      "scores union des techniques\n",
      "f1score : 0.7128463476070528  ; precision :  0.5694164989939637  ; recall :  0.9528619528619529\n",
      "scores intersection des techniques\n",
      "f1score : 0.916058394160584  ; precision :  1.0  ; recall :  0.8451178451178452\n",
      "scores intersection SVM LOF\n",
      "f1score : 0.916058394160584  ; precision :  1.0  ; recall :  0.8451178451178452\n",
      "scores intersection Isolation Forest LOF\n",
      "f1score : 0.8258064516129032  ; precision :  0.7925696594427245  ; recall :  0.8619528619528619\n",
      "scores intersection SVM Isolation Forest\n",
      "f1score : 0.9297297297297298  ; precision :  1.0  ; recall :  0.8686868686868687\n",
      "scores union SVM LOF\n",
      "f1score : 0.7513368983957218  ; precision :  0.623059866962306  ; recall :  0.9461279461279462\n",
      "scores union Isolation Forest LOF\n",
      "f1score : 0.7128463476070528  ; precision :  0.5694164989939637  ; recall :  0.9528619528619529\n",
      "scores union SVM Isolation Forest\n",
      "f1score : 0.787518573551263  ; precision :  0.7047872340425532  ; recall :  0.8922558922558923\n"
     ]
    }
   ],
   "source": [
    "anoSVM,anoLOF,anoISO=detection_anomaly(features_centered [:1677,[7,9]],features_centered [1677:,[7,9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1917, 61441) (1917, 61441)\n",
      "117016632 16262054\n"
     ]
    }
   ],
   "source": [
    "wf = \"haar\"\n",
    "\n",
    "Coeff_test = []\n",
    "TCoeff_test = []\n",
    "for i in range(1917) :\n",
    "    #Apply wavelet decomposition\n",
    "    x=test.iloc[i]\n",
    "    coeffs = pywt.wavedec(x,wf)\n",
    "    coeffs_flatten = np.hstack(coeffs)\n",
    "    Coeff_test.append(coeffs_flatten)\n",
    "    \n",
    "    # Compute universal Threshold http://jseabold.net/blog/2012/02/23/wavelet-regression-in-python/\n",
    "    sigma = mad(coeffs[-1])\n",
    "    uthresh = sigma*np.sqrt(2*np.log(61441))\n",
    "    # Apply Threshold on 4 last levels\n",
    "    coeffs_thresh = [pywt.threshold(c, uthresh, mode=\"hard\") if i<=3 else c for i,c in enumerate(coeffs[::-1])]\n",
    "    coeffs_thresh_flatten = np.hstack(coeffs_thresh[::-1])\n",
    "    TCoeff_test.append(coeffs_thresh_flatten)\n",
    "    \n",
    "Coeff_test  = np.array(Coeff_test )\n",
    "TCoeff_test  = np.array(TCoeff_test )\n",
    "print(Coeff_test .shape, TCoeff_test .shape)\n",
    "print(np.sum(Coeff_test !=0), np.sum(TCoeff_test !=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test= np.zeros((1917,22))\n",
    "for i in range(1917) :\n",
    "    features_test[i,0] = np.min(test.iloc[i])\n",
    "    features_test[i,1] = np.max(test.iloc[i])    \n",
    "    features_test[i,2] = np.median(test.iloc[i])\n",
    "    features_test[i,3] = np.mean(test.iloc[i])\n",
    "    features_test[i,4] = np.var(test.iloc[i])\n",
    "    features_test[i,5] = np.median(test.iloc[i])\n",
    "    features_test[i,6] = np.real(stats.kurtosis(test.iloc[i],bias=False))\n",
    "    features_test[i,7] = np.real(stats.skew(test.iloc[i],bias=False))\n",
    "    features_test[i,10] = np.mean(Coeff_test[i,:1024])\n",
    "    features_test[i,11] = np.var(Coeff_test[i,:1024])\n",
    "    features_test[i,12] = np.median(Coeff_test[i,:1024])\n",
    "    features_test[i,13] = np.real(stats.kurtosis(Coeff_test[i,:1024],bias=False))\n",
    "    features_test[i,14] = np.real(stats.skew(Coeff_test[i,:1024],bias=False))\n",
    "    features_test[i,15] = np.min(Coeff_test[i,:1024])\n",
    "    features_test[i,16] = np.max(Coeff_test[i,:1024])\n",
    "    features_test[i,17] = np.mean(TCoeff_test[i,:1024])\n",
    "    features_test[i,18] = np.var(TCoeff_test[i,:1024])\n",
    "    features_test[i,19] = np.median(TCoeff_test[i,:1024])\n",
    "    features_test[i,20] = np.real(stats.kurtosis(TCoeff_test[i,:1024],bias=False))\n",
    "    features_test[i,21] = np.real(stats.skew(TCoeff_test[i,:1024],bias=False))\n",
    "    features_test[i,8] = np.min(TCoeff_test[i,:1024])\n",
    "    features_test[i,9] = np.max(TCoeff_test[i,:1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/insa/anaconda/envs/GPU/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'anomalies trouvées par SVM 245\n",
      "nombre d'anomalies trouvées par LOF 1000\n",
      "nombre d'anomalies trouvées par Isolation Forest 692\n",
      "scores Isolation Forest\n",
      "f1score : 0.20424671385237614  ; precision :  0.14595375722543352  ; recall :  0.3400673400673401\n",
      "scores SVM\n",
      "f1score : 0.14022140221402216  ; precision :  0.15510204081632653  ; recall :  0.12794612794612795\n",
      "scores LOF\n",
      "f1score : 0.2390131071703932  ; precision :  0.155  ; recall :  0.5218855218855218\n",
      "scores union des techniques\n",
      "f1score : 0.23981552651806304  ; precision :  0.1553784860557769  ; recall :  0.5252525252525253\n",
      "scores intersection des techniques\n",
      "f1score : 0.1367837338262477  ; precision :  0.15163934426229508  ; recall :  0.12457912457912458\n",
      "scores intersection SVM LOF\n",
      "f1score : 0.1367837338262477  ; precision :  0.15163934426229508  ; recall :  0.12457912457912458\n",
      "scores intersection Isolation Forest LOF\n",
      "f1score : 0.20304568527918782  ; precision :  0.14534883720930233  ; recall :  0.3367003367003367\n",
      "scores intersection SVM Isolation Forest\n",
      "f1score : 0.1338432122370937  ; precision :  0.15486725663716813  ; recall :  0.11784511784511785\n",
      "scores union SVM LOF\n",
      "f1score : 0.24036979969183359  ; precision :  0.15584415584415584  ; recall :  0.5252525252525253\n",
      "scores union Isolation Forest LOF\n",
      "f1score : 0.23981552651806304  ; precision :  0.1553784860557769  ; recall :  0.5252525252525253\n",
      "scores union SVM Isolation Forest\n",
      "f1score : 0.20634920634920637  ; precision :  0.14627285513361463  ; recall :  0.3501683501683502\n"
     ]
    }
   ],
   "source": [
    "anoSVM_coeff_test,anoLOF_coeff_test,anoISO_coeff_test=detection_anomaly(Coeff[:1677,:1024],Coeff_test[:,:1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 1, ..., 1, 1, 1]),\n",
       " array([0, 0, 1, ..., 1, 1, 0]))"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anoSVM_coeff_test,anoLOF_coeff_test,anoISO_coeff_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(anoSVM_coeff_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "688"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.intersect1d(np.where(anoISO_coeff_test),np.where(anoLOF_coeff_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=np.intersect1d(np.where(anoLOF_coeff_test),np.intersect1d(np.where(anoISO_coeff_test),np.where(anoSVM_coeff_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[1 if nb in index else 0 for nb in range(1917)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=pd.DataFrame({'seqID':range(1917),'anomaly':result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seqID      1836486\n",
       "anomaly        225\n",
       "dtype: int64"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.to_csv('intersect_all_method_coeff.csv',sep = ';', mode = 'w',index=False,header=['seqID','anomaly'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/insa/anaconda/envs/GPU/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'anomalies trouvées par SVM 498\n",
      "nombre d'anomalies trouvées par LOF 1309\n",
      "nombre d'anomalies trouvées par Isolation Forest 1608\n",
      "scores Isolation Forest\n",
      "f1score : 0.2572178477690289  ; precision :  0.15236318407960198  ; recall :  0.8249158249158249\n",
      "scores SVM\n",
      "f1score : 0.17106918238993712  ; precision :  0.13654618473895583  ; recall :  0.22895622895622897\n",
      "scores LOF\n",
      "f1score : 0.2440846824408468  ; precision :  0.1497326203208556  ; recall :  0.6599326599326599\n",
      "scores union des techniques\n",
      "f1score : 0.2669245647969052  ; precision :  0.15584415584415584  ; recall :  0.9292929292929293\n",
      "scores intersection des techniques\n",
      "f1score : 0.16056338028169012  ; precision :  0.13801452784503632  ; recall :  0.1919191919191919\n",
      "scores intersection SVM LOF\n",
      "f1score : 0.16056338028169012  ; precision :  0.13801452784503632  ; recall :  0.1919191919191919\n",
      "scores intersection Isolation Forest LOF\n",
      "f1score : 0.2286902286902287  ; precision :  0.14397905759162305  ; recall :  0.5555555555555556\n",
      "scores intersection SVM Isolation Forest\n",
      "f1score : 0.17106918238993712  ; precision :  0.13654618473895583  ; recall :  0.22895622895622897\n",
      "scores union SVM LOF\n",
      "f1score : 0.24482554701360143  ; precision :  0.148493543758967  ; recall :  0.696969696969697\n",
      "scores union Isolation Forest LOF\n",
      "f1score : 0.2669245647969052  ; precision :  0.15584415584415584  ; recall :  0.9292929292929293\n",
      "scores union SVM Isolation Forest\n",
      "f1score : 0.2572178477690289  ; precision :  0.15236318407960198  ; recall :  0.8249158249158249\n"
     ]
    }
   ],
   "source": [
    "anoSVM_feature_test,anoLOF,anoISO=detection_anomaly(features[:1677,[14,15,18]],features_test[:,[0,1,2]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(anoSVM_feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=anoSVM_feature_test\n",
    "\n",
    "sum(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=pd.DataFrame({'seqID':range(1917),'anomaly':res})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.to_csv('svmOnfeaturemoyenneminmax.csv',sep = ';', mode = 'w',index=False,header=['seqID','anomaly'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anoSVM_feature_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "listoutliersintersect=[1 if number in np.intersect1d(np.where(anoSVM_coeff_centered),np.where(anoSVM_features_all_data)) else 0 for number in range(594)]\n",
    "listoutliersunion=[1 if number in np.union1d(np.where(anoSVM_coeff_centered),np.where(anoSVM_features_all_data)) else 0 for number in range(594)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(listoutliersintersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(listoutliersunion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listoutliersunion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1score : 0.9373881932021467  ; precision :  1.0  ; recall :  0.8821548821548821\n"
     ]
    }
   ],
   "source": [
    "scores(np.where(listoutliersintersect)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_centered=test.apply(lambda row: (row-np.mean(row))/np.var(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1917, 61441) (1917, 61441)\n",
      "117780480 16276025\n"
     ]
    }
   ],
   "source": [
    "wf = \"haar\"\n",
    "\n",
    "Coeff_centered_test = []\n",
    "TCoeff_centered_test = []\n",
    "for i in range(1917) :\n",
    "    #Apply wavelet decomposition\n",
    "    x=test_centered.iloc[i]\n",
    "    coeffs = pywt.wavedec(x,wf)\n",
    "    coeffs_flatten = np.hstack(coeffs)\n",
    "    Coeff_centered_test.append(coeffs_flatten)\n",
    "    \n",
    "    # Compute universal Threshold http://jseabold.net/blog/2012/02/23/wavelet-regression-in-python/\n",
    "    sigma = mad(coeffs[-1])\n",
    "    uthresh = sigma*np.sqrt(2*np.log(61441))\n",
    "    # Apply Threshold on 4 last levels\n",
    "    coeffs_thresh = [pywt.threshold(c, uthresh, mode=\"hard\") if i<=3 else c for i,c in enumerate(coeffs[::-1])]\n",
    "    coeffs_thresh_flatten = np.hstack(coeffs_thresh[::-1])\n",
    "    TCoeff_centered_test.append(coeffs_thresh_flatten)\n",
    "    \n",
    "Coeff_centered_test  = np.array(Coeff_centered_test )\n",
    "TCoeff_centered_test  = np.array(TCoeff_centered_test )\n",
    "print(Coeff_centered_test .shape, TCoeff_centered_test .shape)\n",
    "print(np.sum(Coeff_centered_test !=0), np.sum(TCoeff_centered_test !=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/insa/anaconda/envs/GPU/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'anomalies trouvées par SVM 1917\n",
      "nombre d'anomalies trouvées par LOF 1917\n",
      "nombre d'anomalies trouvées par Isolation Forest 1902\n",
      "scores Isolation Forest\n",
      "f1score : 0.2664847658026376  ; precision :  0.15404837013669823  ; recall :  0.9865319865319865\n",
      "scores SVM\n",
      "f1score : 0.26829268292682923  ; precision :  0.15492957746478872  ; recall :  1.0\n",
      "scores LOF\n",
      "f1score : 0.26829268292682923  ; precision :  0.15492957746478872  ; recall :  1.0\n",
      "scores union des techniques\n",
      "f1score : 0.26829268292682923  ; precision :  0.15492957746478872  ; recall :  1.0\n",
      "scores intersection des techniques\n",
      "f1score : 0.26829268292682923  ; precision :  0.15492957746478872  ; recall :  1.0\n",
      "scores intersection SVM LOF\n",
      "f1score : 0.26829268292682923  ; precision :  0.15492957746478872  ; recall :  1.0\n",
      "scores intersection Isolation Forest LOF\n",
      "f1score : 0.2664847658026376  ; precision :  0.15404837013669823  ; recall :  0.9865319865319865\n",
      "scores intersection SVM Isolation Forest\n",
      "f1score : 0.2664847658026376  ; precision :  0.15404837013669823  ; recall :  0.9865319865319865\n",
      "scores union SVM LOF\n",
      "f1score : 0.26829268292682923  ; precision :  0.15492957746478872  ; recall :  1.0\n",
      "scores union Isolation Forest LOF\n",
      "f1score : 0.26829268292682923  ; precision :  0.15492957746478872  ; recall :  1.0\n",
      "scores union SVM Isolation Forest\n",
      "f1score : 0.26829268292682923  ; precision :  0.15492957746478872  ; recall :  1.0\n"
     ]
    }
   ],
   "source": [
    "anoSVM,anoLOF,anoISO=detection_anomaly(Coeff_centered[:1677,:1024],Coeff_centered_test[:,:1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_centered_test= np.zeros((1917,21))\n",
    "for i in range(1917) : \n",
    "    features_centered_test[i,0] = np.mean(test_centered.iloc[i])\n",
    "    features_centered_test[i,1] = np.var(test_centered.iloc[i])\n",
    "    features_centered_test[i,2] = np.median(test_centered.iloc[i])\n",
    "    features_centered_test[i,3] = np.real(stats.kurtosis(test_centered.iloc[i],bias=False))\n",
    "    features_centered_test[i,4] = np.real(stats.skew(test_centered.iloc[i],bias=False))\n",
    "    features_centered_test[i,5] = np.min(test_centered.iloc[i])\n",
    "    features_centered_test[i,6] = np.max(test_centered.iloc[i])\n",
    "    features_centered_test[i,7] = np.mean(Coeff_centered[i,:1024])\n",
    "    features_centered_test[i,8] = np.var(Coeff_centered[i,:1024])\n",
    "    features_centered_test[i,9] = np.median(Coeff_centered[i,:1024])\n",
    "    features_centered_test[i,10] = np.real(stats.kurtosis(Coeff_centered[i,:1024],bias=False))\n",
    "    features_centered_test[i,11] = np.real(stats.skew(Coeff_centered[i,:1024],bias=False))\n",
    "    features_centered_test[i,12] = np.min(Coeff_centered[i,:1024])\n",
    "    features_centered_test[i,13] = np.max(Coeff_centered[i,:1024])\n",
    "    features_centered_test[i,14] = np.mean(TCoeff_centered[i,:1024])\n",
    "    features_centered_test[i,15] = np.var(TCoeff_centered[i,:1024])\n",
    "    features_centered_test[i,16] = np.median(TCoeff_centered[i,:1024])\n",
    "    features_centered_test[i,17] = np.real(stats.kurtosis(TCoeff_centered[i,:1024],bias=False))\n",
    "    features_centered_test[i,18] = np.real(stats.skew(TCoeff_centered[i,:1024],bias=False))\n",
    "    features_centered_test[i,19] = np.min(TCoeff_centered[i,:1024])\n",
    "    features_centered_test[i,20] = np.max(TCoeff_centered[i,:1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/insa/anaconda/envs/GPU/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'anomalies trouvées par SVM 178\n",
      "nombre d'anomalies trouvées par LOF 777\n",
      "nombre d'anomalies trouvées par Isolation Forest 985\n",
      "scores Isolation Forest\n",
      "f1score : 0.24336973478939156  ; precision :  0.15837563451776648  ; recall :  0.5252525252525253\n",
      "scores SVM\n",
      "f1score : 0.029473684210526315  ; precision :  0.03932584269662921  ; recall :  0.02356902356902357\n",
      "scores LOF\n",
      "f1score : 0.18435754189944134  ; precision :  0.1274131274131274  ; recall :  0.3333333333333333\n",
      "scores union des techniques\n",
      "f1score : 0.25157232704402516  ; precision :  0.15467904098994587  ; recall :  0.6734006734006734\n",
      "scores intersection des techniques\n",
      "f1score : 0.018140589569160998  ; precision :  0.027777777777777776  ; recall :  0.013468013468013467\n",
      "scores intersection SVM LOF\n",
      "f1score : 0.018140589569160998  ; precision :  0.027777777777777776  ; recall :  0.013468013468013467\n",
      "scores intersection Isolation Forest LOF\n",
      "f1score : 0.14360313315926893  ; precision :  0.11727078891257996  ; recall :  0.18518518518518517\n",
      "scores intersection SVM Isolation Forest\n",
      "f1score : 0.029473684210526315  ; precision :  0.03932584269662921  ; recall :  0.02356902356902357\n",
      "scores union SVM LOF\n",
      "f1score : 0.18411552346570395  ; precision :  0.12577065351418001  ; recall :  0.3434343434343434\n",
      "scores union Isolation Forest LOF\n",
      "f1score : 0.25157232704402516  ; precision :  0.15467904098994587  ; recall :  0.6734006734006734\n",
      "scores union SVM Isolation Forest\n",
      "f1score : 0.24336973478939156  ; precision :  0.15837563451776648  ; recall :  0.5252525252525253\n"
     ]
    }
   ],
   "source": [
    "anoSVM_fetures_centered_test,anoLOF_fetures_centered_test,anoISO_fetures_centered_test=detection_anomaly(features_centered[:1677,[7,9]],features_centered_test[:,[7,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anoSVM_fetures_centered_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(anoSVM_feature_test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.intersect1d(np.where(anoSVM_feature_test),np.where(anomalyautoencoder)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_centered_mean=all_data.apply(lambda row: (row-np.mean(row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2271, 61441) (2271, 61441)\n",
      "139530240 15752235\n"
     ]
    }
   ],
   "source": [
    "wf = \"haar\"\n",
    "\n",
    "Coeff_centered_mean = []\n",
    "TCoeff_centered_mean = []\n",
    "for i in range(2271) :\n",
    "    #Apply wavelet decomposition\n",
    "    x=all_data_centered_mean.iloc[i]\n",
    "    coeffs = pywt.wavedec(x,wf)\n",
    "    coeffs_flatten = np.hstack(coeffs)\n",
    "    Coeff_centered_mean.append(coeffs_flatten)\n",
    "    \n",
    "    # Compute universal Threshold http://jseabold.net/blog/2012/02/23/wavelet-regression-in-python/\n",
    "    sigma = mad(coeffs[-1])\n",
    "    uthresh = sigma*np.sqrt(2*np.log(61441))\n",
    "    # Apply Threshold on 4 last levels\n",
    "    coeffs_thresh = [pywt.threshold(c, uthresh, mode=\"hard\") if i<=3 else c for i,c in enumerate(coeffs[::-1])]\n",
    "    coeffs_thresh_flatten = np.hstack(coeffs_thresh[::-1])\n",
    "    TCoeff_centered_mean.append(coeffs_thresh_flatten)\n",
    "    \n",
    "Coeff_centered_mean  = np.array(Coeff_centered_mean )\n",
    "TCoeff_centered_mean  = np.array(TCoeff_centered_mean )\n",
    "print(Coeff_centered_mean .shape, TCoeff_centered_mean .shape)\n",
    "print(np.sum(Coeff_centered_mean !=0), np.sum(TCoeff_centered_mean !=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/insa/anaconda/envs/GPU/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'anomalies trouvées par SVM 261\n",
      "nombre d'anomalies trouvées par LOF 321\n",
      "nombre d'anomalies trouvées par Isolation Forest 286\n",
      "scores Isolation Forest\n",
      "f1score : 0.5660377358490566  ; precision :  0.5769230769230769  ; recall :  0.5555555555555556\n",
      "scores SVM\n",
      "f1score : 0.9247311827956989  ; precision :  0.9885057471264368  ; recall :  0.8686868686868687\n",
      "scores LOF\n",
      "f1score : 0.7411003236245954  ; precision :  0.7133956386292835  ; recall :  0.7710437710437711\n",
      "scores union des techniques\n",
      "f1score : 0.7337110481586401  ; precision :  0.6332518337408313  ; recall :  0.8720538720538721\n",
      "scores intersection des techniques\n",
      "f1score : 0.870722433460076  ; precision :  1.0  ; recall :  0.7710437710437711\n",
      "scores intersection SVM LOF\n",
      "f1score : 0.870722433460076  ; precision :  1.0  ; recall :  0.7710437710437711\n",
      "scores intersection Isolation Forest LOF\n",
      "f1score : 0.5454545454545454  ; precision :  0.6818181818181818  ; recall :  0.45454545454545453\n",
      "scores intersection SVM Isolation Forest\n",
      "f1score : 0.706896551724138  ; precision :  0.9820359281437125  ; recall :  0.5521885521885522\n",
      "scores union SVM LOF\n",
      "f1score : 0.7938461538461539  ; precision :  0.7308781869688386  ; recall :  0.8686868686868687\n",
      "scores union Isolation Forest LOF\n",
      "f1score : 0.7337110481586401  ; precision :  0.6332518337408313  ; recall :  0.8720538720538721\n",
      "scores union SVM Isolation Forest\n",
      "f1score : 0.7651403249630725  ; precision :  0.6815789473684211  ; recall :  0.8720538720538721\n"
     ]
    }
   ],
   "source": [
    "anoSVM_coeff_centered,anoLOF,anoISO=detection_anomaly(Coeff_centered_mean [:1677,:1024],Coeff_centered_mean [1677:,:1024])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow\n",
    "tensorflow.__version__\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import tensorflow.keras.preprocessing.image as kpi\n",
    "import tensorflow.keras.models as km\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.losses as kloss\n",
    "import tensorflow.keras.regularizers as kr\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.utils as ku\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set_style(\"whitegrid\")\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.layers import Embedding,LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = f.iloc[:1174]\n",
    "x_validation = f.iloc[1174:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "epochs = 20\n",
    "n_latent = 32\n",
    "n_input = 61440\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = km.Sequential(name=\"EncoderModel\")\n",
    "encoder.add(kl.Dense(n_latent, activation='relu', input_shape=(n_input,),name=\"encoder_layer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = km.Sequential(name=\"DecoderModel\")\n",
    "decoder.add(kl.Dense(n_input, activation='sigmoid', input_shape =(n_latent,), name = \"decoded_layer\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder = km.Sequential(name=\"EncoderModel\")\n",
    "encoder.add(kl.GRU(256, input_shape=n_input,return_sequences=True,name=\"encoder_layer\"))\n",
    "encoder.add(kl.Bidirectional(kl.GRU(128)))\n",
    "encoder.add(kl.RepeatVector(128))\n",
    "encoder.add(kl.GRU(n_latent,return_sequences=True,activation='relu'))\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decoder = km.Sequential(name=\"DecoderModel\")\n",
    "decoder.add(kl.GRU(128,input_shape=(128,32),name=\"decoder_layer\"))\n",
    "decoder.add(kl.RepeatVector(61440))\n",
    "decoder.add(kl.GRU(256,return_sequences=True))\n",
    "decoder.add(kl.Dense(128, activation='sigmoid' ))\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "EncoderModel (Sequential)    (None, 32)                1966112   \n",
      "_________________________________________________________________\n",
      "DecoderModel (Sequential)    (None, 61440)             2027520   \n",
      "=================================================================\n",
      "Total params: 3,993,632\n",
      "Trainable params: 3,993,632\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder = km.Sequential(name=\"EncoderDecoder\")\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1174 samples, validate on 503 samples\n",
      "Epoch 1/70\n",
      "1174/1174 [==============================] - 2s 2ms/sample - loss: 0.6932 - val_loss: 0.6931\n",
      "Epoch 2/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6931 - val_loss: 0.6931\n",
      "Epoch 3/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6929 - val_loss: 0.6930\n",
      "Epoch 4/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6928 - val_loss: 0.6929\n",
      "Epoch 5/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6927 - val_loss: 0.6928\n",
      "Epoch 6/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6926 - val_loss: 0.6927\n",
      "Epoch 7/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6924 - val_loss: 0.6927\n",
      "Epoch 8/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6922 - val_loss: 0.6926\n",
      "Epoch 9/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6921 - val_loss: 0.6925\n",
      "Epoch 10/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6918 - val_loss: 0.6924\n",
      "Epoch 11/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6916 - val_loss: 0.6923\n",
      "Epoch 12/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6913 - val_loss: 0.6922\n",
      "Epoch 13/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6910 - val_loss: 0.6921\n",
      "Epoch 14/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6906 - val_loss: 0.6919\n",
      "Epoch 15/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6901 - val_loss: 0.6918\n",
      "Epoch 16/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6895 - val_loss: 0.6916\n",
      "Epoch 17/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6889 - val_loss: 0.6915\n",
      "Epoch 18/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6880 - val_loss: 0.6913\n",
      "Epoch 19/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6870 - val_loss: 0.6911\n",
      "Epoch 20/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6858 - val_loss: 0.6908\n",
      "Epoch 21/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6841 - val_loss: 0.6905\n",
      "Epoch 22/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6822 - val_loss: 0.6902\n",
      "Epoch 23/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6798 - val_loss: 0.6898\n",
      "Epoch 24/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6767 - val_loss: 0.6893\n",
      "Epoch 25/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6729 - val_loss: 0.6888\n",
      "Epoch 26/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6680 - val_loss: 0.6881\n",
      "Epoch 27/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6619 - val_loss: 0.6873\n",
      "Epoch 28/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6541 - val_loss: 0.6864\n",
      "Epoch 29/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6445 - val_loss: 0.6853\n",
      "Epoch 30/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6325 - val_loss: 0.6840\n",
      "Epoch 31/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6187 - val_loss: 0.6825\n",
      "Epoch 32/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.6021 - val_loss: 0.6808\n",
      "Epoch 33/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.5846 - val_loss: 0.6789\n",
      "Epoch 34/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.5665 - val_loss: 0.6768\n",
      "Epoch 35/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.5478 - val_loss: 0.6746\n",
      "Epoch 36/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.5284 - val_loss: 0.6723\n",
      "Epoch 37/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.5084 - val_loss: 0.6698\n",
      "Epoch 38/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.4884 - val_loss: 0.6671\n",
      "Epoch 39/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.4689 - val_loss: 0.6643\n",
      "Epoch 40/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.4489 - val_loss: 0.6614\n",
      "Epoch 41/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.4294 - val_loss: 0.6582\n",
      "Epoch 42/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.4106 - val_loss: 0.6549\n",
      "Epoch 43/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.3915 - val_loss: 0.6514\n",
      "Epoch 44/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.3726 - val_loss: 0.6478\n",
      "Epoch 45/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.3542 - val_loss: 0.6440\n",
      "Epoch 46/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.3363 - val_loss: 0.6401\n",
      "Epoch 47/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.3179 - val_loss: 0.6361\n",
      "Epoch 48/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.3003 - val_loss: 0.6317\n",
      "Epoch 49/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.2827 - val_loss: 0.6270\n",
      "Epoch 50/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.2650 - val_loss: 0.6227\n",
      "Epoch 51/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.2472 - val_loss: 0.6176\n",
      "Epoch 52/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.2295 - val_loss: 0.6125\n",
      "Epoch 53/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.2119 - val_loss: 0.6071\n",
      "Epoch 54/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.1948 - val_loss: 0.6015\n",
      "Epoch 55/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.1774 - val_loss: 0.5957\n",
      "Epoch 56/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.1606 - val_loss: 0.5897\n",
      "Epoch 57/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.1431 - val_loss: 0.5834\n",
      "Epoch 58/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.1262 - val_loss: 0.5775\n",
      "Epoch 59/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.1088 - val_loss: 0.5706\n",
      "Epoch 60/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.0923 - val_loss: 0.5640\n",
      "Epoch 61/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.0749 - val_loss: 0.5569\n",
      "Epoch 62/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.0581 - val_loss: 0.5504\n",
      "Epoch 63/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.0411 - val_loss: 0.5433\n",
      "Epoch 64/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.0242 - val_loss: 0.5361\n",
      "Epoch 65/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: 0.0075 - val_loss: 0.5284\n",
      "Epoch 66/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: -0.0091 - val_loss: 0.5212\n",
      "Epoch 67/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: -0.0254 - val_loss: 0.5136\n",
      "Epoch 68/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: -0.0418 - val_loss: 0.5062\n",
      "Epoch 69/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: -0.0580 - val_loss: 0.4984\n",
      "Epoch 70/70\n",
      "1174/1174 [==============================] - 1s 1ms/sample - loss: -0.0744 - val_loss: 0.4907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb84437ba20>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "autoencoder.fit(x_train, x_train, epochs=70, batch_size=300, validation_data=(x_validation, x_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(594, 32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_imgs = encoder.predict(v)\n",
    "encoded_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(594, 61440)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_imgs = decoder.predict(encoded_imgs)\n",
    "decoded_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = np.zeros(594)\n",
    "for i in range(594):\n",
    "    distance[i] = np.sum((decoded_imgs[i] - v.iloc[i])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33246.43665570937"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mediane1st = np.median(distance)\n",
    "mediane1st "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
       "       1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
       "       1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
       "       0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
       "       0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
       "       1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1.])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomalyautoencoder = np.zeros(594)\n",
    "for i in range(594):\n",
    "    if distance[i]>mediane1st:\n",
    "        anomalyautoencoder[i] = 1\n",
    "anomalyautoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=autoencoder.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = np.zeros(1917)\n",
    "for i in range(1917):\n",
    "    distance[i] = np.sum((pred[i] - test.iloc[i])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1917"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomalyautoencoder = np.zeros(1917)\n",
    "for i in range(1917):\n",
    "    if distance[i]>mediane1st:\n",
    "        anomalyautoencoder[i] = 1\n",
    "len(anomalyautoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "658.0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(anomalyautoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   2,    6,    7,    9,   10,   20,   23,   24,   28,   35,   37,\n",
       "          38,   39,   40,   42,   46,   47,   48,   49,   51,   54,   61,\n",
       "          62,   68,   69,   70,   72,   73,   74,   75,   78,   82,   92,\n",
       "          96,   97,   99,  101,  111,  112,  113,  115,  117,  118,  120,\n",
       "         122,  124,  127,  138,  139,  145,  147,  150,  151,  152,  153,\n",
       "         156,  159,  160,  161,  163,  165,  167,  169,  173,  175,  177,\n",
       "         185,  188,  189,  195,  198,  205,  210,  214,  216,  219,  220,\n",
       "         221,  222,  223,  225,  227,  230,  238,  240,  242,  243,  248,\n",
       "         249,  253,  254,  255,  256,  261,  262,  265,  266,  268,  271,\n",
       "         275,  277,  282,  284,  288,  299,  301,  308,  309,  315,  318,\n",
       "         319,  321,  322,  323,  326,  330,  337,  338,  343,  347,  348,\n",
       "         350,  358,  360,  364,  365,  366,  367,  369,  372,  373,  374,\n",
       "         376,  377,  378,  382,  383,  385,  386,  390,  393,  395,  398,\n",
       "         403,  405,  411,  414,  420,  422,  429,  432,  434,  435,  438,\n",
       "         440,  441,  442,  444,  447,  448,  449,  453,  458,  462,  463,\n",
       "         465,  466,  473,  475,  480,  491,  493,  497,  500,  503,  504,\n",
       "         506,  508,  511,  520,  522,  529,  531,  534,  535,  538,  539,\n",
       "         541,  547,  549,  551,  557,  558,  560,  566,  569,  570,  573,\n",
       "         574,  576,  579,  580,  583,  585,  587,  591,  595,  596,  597,\n",
       "         601,  602,  609,  612,  614,  618,  621,  622,  624,  627,  629,\n",
       "         630,  631,  641,  646,  648,  650,  654,  663,  666,  667,  668,\n",
       "         671,  675,  678,  679,  686,  687,  691,  693,  694,  696,  700,\n",
       "         701,  703,  704,  706,  709,  712,  714,  721,  724,  730,  734,\n",
       "         735,  737,  741,  758,  759,  760,  761,  764,  766,  773,  775,\n",
       "         776,  777,  779,  780,  785,  787,  789,  792,  793,  794,  795,\n",
       "         796,  797,  800,  801,  806,  809,  812,  813,  814,  819,  822,\n",
       "         825,  826,  829,  840,  841,  847,  853,  854,  856,  858,  861,\n",
       "         863,  866,  868,  871,  873,  874,  875,  877,  878,  879,  883,\n",
       "         884,  887,  889,  892,  894,  896,  901,  906,  911,  913,  917,\n",
       "         920,  921,  926,  928,  931,  932,  934,  937,  941,  947,  951,\n",
       "         952,  956,  961,  962,  963,  965,  966,  973,  974,  978,  979,\n",
       "         980,  983,  984,  990,  993, 1003, 1005, 1008, 1017, 1020, 1025,\n",
       "        1026, 1027, 1030, 1035, 1041, 1042, 1044, 1047, 1048, 1055, 1056,\n",
       "        1059, 1060, 1062, 1069, 1075, 1078, 1085, 1094, 1098, 1101, 1102,\n",
       "        1104, 1106, 1107, 1108, 1110, 1111, 1118, 1119, 1120, 1121, 1124,\n",
       "        1125, 1127, 1129, 1134, 1138, 1139, 1142, 1148, 1150, 1151, 1160,\n",
       "        1161, 1170, 1171, 1176, 1178, 1179, 1192, 1196, 1201, 1203, 1206,\n",
       "        1213, 1220, 1221, 1223, 1226, 1227, 1229, 1232, 1234, 1236, 1240,\n",
       "        1242, 1243, 1246, 1248, 1249, 1252, 1259, 1262, 1263, 1268, 1269,\n",
       "        1270, 1272, 1273, 1275, 1281, 1282, 1283, 1287, 1288, 1291, 1296,\n",
       "        1298, 1303, 1314, 1317, 1318, 1319, 1321, 1322, 1323, 1324, 1325,\n",
       "        1330, 1331, 1334, 1336, 1341, 1345, 1346, 1347, 1350, 1351, 1352,\n",
       "        1356, 1359, 1363, 1365, 1366, 1367, 1370, 1371, 1373, 1374, 1375,\n",
       "        1376, 1377, 1387, 1390, 1392, 1394, 1397, 1398, 1400, 1402, 1404,\n",
       "        1410, 1411, 1418, 1421, 1436, 1438, 1445, 1448, 1459, 1461, 1466,\n",
       "        1469, 1473, 1475, 1476, 1481, 1483, 1486, 1489, 1490, 1492, 1496,\n",
       "        1497, 1498, 1500, 1502, 1508, 1509, 1514, 1518, 1521, 1524, 1526,\n",
       "        1528, 1530, 1531, 1533, 1537, 1539, 1540, 1541, 1545, 1548, 1553,\n",
       "        1555, 1558, 1561, 1564, 1566, 1569, 1571, 1574, 1578, 1579, 1581,\n",
       "        1582, 1583, 1587, 1592, 1596, 1607, 1611, 1612, 1613, 1618, 1621,\n",
       "        1623, 1624, 1627, 1628, 1630, 1633, 1636, 1639, 1640, 1643, 1646,\n",
       "        1652, 1654, 1655, 1658, 1661, 1667, 1669, 1672, 1673, 1677, 1678,\n",
       "        1680, 1683, 1684, 1685, 1688, 1695, 1697, 1698, 1699, 1702, 1708,\n",
       "        1711, 1712, 1715, 1721, 1725, 1727, 1728, 1730, 1733, 1735, 1739,\n",
       "        1741, 1742, 1744, 1746, 1749, 1750, 1753, 1755, 1757, 1758, 1765,\n",
       "        1771, 1777, 1779, 1784, 1785, 1786, 1788, 1789, 1794, 1797, 1799,\n",
       "        1800, 1802, 1806, 1814, 1817, 1819, 1825, 1829, 1834, 1838, 1839,\n",
       "        1840, 1847, 1852, 1853, 1854, 1855, 1858, 1859, 1861, 1862, 1864,\n",
       "        1865, 1870, 1872, 1874, 1877, 1881, 1882, 1884, 1888, 1891, 1892,\n",
       "        1894, 1895, 1896, 1899, 1900, 1905, 1907, 1914, 1915]),)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(anomalyautoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.intersect1d(np.where(anoSVM_feature_test),np.where(anomalyautoencoder)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1score : 0.8417508417508418  ; precision :  0.8417508417508418  ; recall :  0.8417508417508418\n"
     ]
    }
   ],
   "source": [
    "scores(np.where(anomalyautoencoder)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# removing outliers on train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1677, 61441) (1677, 61441)\n",
      "102910729 11514957\n"
     ]
    }
   ],
   "source": [
    "wf = \"haar\"\n",
    "\n",
    "Coeff_rawdata = []\n",
    "TCoeff_rawdata = []\n",
    "for i in range(1677) :\n",
    "    #Apply wavelet decomposition\n",
    "    x=f.iloc[i]\n",
    "    coeffs = pywt.wavedec(x,wf)\n",
    "    coeffs_flatten = np.hstack(coeffs)\n",
    "    Coeff_rawdata.append(coeffs_flatten)\n",
    "    \n",
    "    # Compute universal Threshold http://jseabold.net/blog/2012/02/23/wavelet-regression-in-python/\n",
    "    sigma = mad(coeffs[-1])\n",
    "    uthresh = sigma*np.sqrt(2*np.log(61441))\n",
    "    # Apply Threshold on 4 last levels\n",
    "    coeffs_thresh = [pywt.threshold(c, uthresh, mode=\"hard\") if i<=3 else c for i,c in enumerate(coeffs[::-1])]\n",
    "    coeffs_thresh_flatten = np.hstack(coeffs_thresh[::-1])\n",
    "    TCoeff_rawdata.append(coeffs_thresh_flatten)\n",
    "    \n",
    "Coeff_rawdata = np.array(Coeff_rawdata)\n",
    "TCoeff_rawdata = np.array(TCoeff_rawdata)\n",
    "print(Coeff_rawdata.shape, TCoeff_rawdata.shape)\n",
    "print(np.sum(Coeff_rawdata!=0), np.sum(TCoeff_rawdata!=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_anomaly_rawdata(data):\n",
    "    OCS = ssvm.OneClassSVM(kernel=\"rbf\", nu=0.05)\n",
    "\n",
    "    anoSVM=OCS.fit_predict(data)\n",
    "     \n",
    "\n",
    "    for i in range(len(anoSVM)) : \n",
    "        if anoSVM[i]==1 : \n",
    "            anoSVM[i]=0\n",
    "        else :\n",
    "            anoSVM[i]= 1 \n",
    "    print(\"nombre d'anomalies trouvées par SVM\",sum(anoSVM))\n",
    "\n",
    "    contamination=0.10\n",
    "    metric = \"euclidean\"\n",
    "    clf = sn.LocalOutlierFactor(n_neighbors=5, contamination=contamination, metric=metric)\n",
    "    anoLOF = clf.fit_predict(data)\n",
    "    \n",
    "\n",
    "    for i in range(len(anoLOF)) : \n",
    "        if anoLOF[i]==1 : \n",
    "            anoLOF[i]=0\n",
    "        else :\n",
    "            anoLOF[i]= 1 \n",
    "    \n",
    "    print(\"nombre d'anomalies trouvées par LOF\",sum(anoLOF))\n",
    "\n",
    "    clf = IsolationForest(contamination=0.10)\n",
    "    anoISO=clf.fit_predict(data)\n",
    "    \n",
    "\n",
    "    for i in range(len(anoISO)) : \n",
    "        if anoISO[i]==1 : \n",
    "            anoISO[i]=0\n",
    "        else :\n",
    "            anoISO[i]= 1 \n",
    "    print(\"nombre d'anomalies trouvées par Isolation Forest\",sum(anoISO))\n",
    "    \n",
    "    listoutliersintersect=np.intersect1d(np.where(anoLOF==1),np.where(anoSVM==1),np.where(anoISO==1))\n",
    "    listoutliersunion=np.union1d(np.union1d(np.where(anoLOF==1),np.where(anoSVM==1)),np.where(anoISO==1))\n",
    "\n",
    "    return listoutliersintersect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebas\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'anomalies trouvées par SVM 96\n",
      "nombre d'anomalies trouvées par LOF 168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebas\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:247: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\sebas\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:415: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'anomalies trouvées par Isolation Forest 168\n"
     ]
    }
   ],
   "source": [
    "listoutliers=detection_anomaly_rawdata(Coeff_rawdata[:,:1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 303,  359,  455,  466,  472,  707,  727,  761,  777,  832,  867,\n",
       "        896,  931,  936,  977, 1071, 1076, 1077, 1111, 1112, 1251, 1287,\n",
       "       1356, 1357, 1372, 1391, 1427, 1461, 1462, 1572, 1615], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listoutliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "listinliers=[]\n",
    "for i in range(len(Coeff_rawdata[:,:1024])):\n",
    "    if i not in listoutliers:\n",
    "        listinliers.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Coeff_rawdata[listoutliers,:1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = sd.PCA(n_components=4)\n",
    "scaler = sp.scale(Coeff_rawdata[:,:1024])\n",
    "acp = pca.fit_transform(scaler)\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.show()\n",
    "plt.boxplot(acp)\n",
    "plt.show()\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(acp[listinliers,1], acp[listinliers,2],color=\"blue\")\n",
    "plt.scatter(acp[listoutliers,1], acp[listoutliers,2],color=\"red\")\n",
    "#plt.xlim(-limx,limx)\n",
    "#plt.ylim(-limy,limy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebas\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'anomalies trouvées par SVM 261\n",
      "nombre d'anomalies trouvées par LOF 322\n",
      "nombre d'anomalies trouvées par Isolation Forest 296\n",
      "scores Isolation Forest\n",
      "f1score : 0.6062717770034843  ; precision :  0.5878378378378378  ; recall :  0.6258992805755396\n",
      "scores SVM\n",
      "f1score : 0.953617810760668  ; precision :  0.9846743295019157  ; recall :  0.9244604316546763\n",
      "scores LOF\n",
      "f1score : 0.77  ; precision :  0.717391304347826  ; recall :  0.8309352517985612\n",
      "scores union des techniques\n",
      "f1score : 0.7561327561327561  ; precision :  0.6313253012048192  ; recall :  0.9424460431654677\n",
      "scores intersection des techniques\n",
      "f1score : 0.8994082840236687  ; precision :  0.9956331877729258  ; recall :  0.8201438848920863\n",
      "scores intersection SVM LOF\n",
      "f1score : 0.8994082840236687  ; precision :  0.9956331877729258  ; recall :  0.8201438848920863\n",
      "scores intersection Isolation Forest LOF\n",
      "f1score : 0.5945945945945946  ; precision :  0.7044334975369458  ; recall :  0.5143884892086331\n",
      "scores intersection SVM Isolation Forest\n",
      "f1score : 0.7494456762749446  ; precision :  0.976878612716763  ; recall :  0.6079136690647482\n",
      "scores union SVM LOF\n",
      "f1score : 0.8227848101265822  ; precision :  0.7344632768361582  ; recall :  0.935251798561151\n",
      "scores union Isolation Forest LOF\n",
      "f1score : 0.7561327561327561  ; precision :  0.6313253012048192  ; recall :  0.9424460431654677\n",
      "scores union SVM Isolation Forest\n",
      "f1score : 0.7915407854984894  ; precision :  0.6822916666666666  ; recall :  0.9424460431654677\n"
     ]
    }
   ],
   "source": [
    "anoSVM,anoLOF,anoISO=detection_anomaly(Coeff[listinliers,:1024],Coeff[1677:,:1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import tensorflow.keras.preprocessing.image as kpi\n",
    "import tensorflow.keras.models as km\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.losses as kloss\n",
    "import tensorflow.keras.regularizers as kr\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.utils as ku\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim))\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "intermediate_dim = 512\n",
    "latent_dim = 2\n",
    "epochs = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = kl.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 1024)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 512)          524800      encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            1026        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            1026        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (100, 2)             0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 526,852\n",
      "Trainable params: 526,852\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# instantiate encoder model\n",
    "\n",
    "encoder = km.Model(inputs, z, name='encoder')\n",
    "encoder.summary()\n",
    "#ku.plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/decoder_vae.py\n",
    "# build decoder model\n",
    "latent_inputs = kl.Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = kl.Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = kl.Dense(1024, activation='sigmoid')(x)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = km.Model(latent_inputs, outputs, name='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               1536      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1024)              525312    \n",
      "=================================================================\n",
      "Total params: 526,848\n",
      "Trainable params: 526,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae_mlp\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 1024)]            0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              (100, 2)                  526852    \n",
      "_________________________________________________________________\n",
      "decoder (Model)              multiple                  526848    \n",
      "=================================================================\n",
      "Total params: 1,053,700\n",
      "Trainable params: 1,053,700\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs))\n",
    "vae = km.Model(inputs, outputs, name='vae_mlp')\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output decoder missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to decoder.\n"
     ]
    }
   ],
   "source": [
    "reconstruction_loss = kloss.binary_crossentropy(inputs,\n",
    "                                              outputs)\n",
    "\n",
    "reconstruction_loss *= 784\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1300 samples\n",
      "Epoch 1/5\n",
      "1300/1300 [==============================] - 0s 245us/sample - loss: -313.1605\n",
      "Epoch 2/5\n",
      "1300/1300 [==============================] - 0s 252us/sample - loss: -329.5684\n",
      "Epoch 3/5\n",
      "1300/1300 [==============================] - 0s 237us/sample - loss: -337.7395\n",
      "Epoch 4/5\n",
      "1300/1300 [==============================] - 0s 247us/sample - loss: -349.4649\n",
      "Epoch 5/5\n",
      "1300/1300 [==============================] - 0s 232us/sample - loss: -361.9095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f2d0a83148>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.fit(Coeff[:1300,:1024],epochs=5,verbose=True, batch_size=batch_size,use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1300 samples\n",
      "Epoch 1/5\n",
      "1300/1300 [==============================] - 2s 1ms/sample - loss: 1295.4525\n",
      "Epoch 2/5\n",
      "1300/1300 [==============================] - 0s 235us/sample - loss: 415.7099\n",
      "Epoch 3/5\n",
      "1300/1300 [==============================] - 0s 245us/sample - loss: 184.4477\n",
      "Epoch 4/5\n",
      "1300/1300 [==============================] - 0s 251us/sample - loss: 54.8851\n",
      "Epoch 5/5\n",
      "1300/1300 [==============================] - 0s 243us/sample - loss: -16.0670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f2a3b72ec8>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_ad.fit(Coeff[:1300,:1024],epochs=5, batch_size=batch_size, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": " Incompatible shapes: [94,2] vs. [100,2]\n\t [[node vae_mlp/encoder/z/mul (defined at C:\\Users\\sebas\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]] [Op:__inference_distributed_function_5663]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-7243bb24a1ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_test_decoded_vae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvae_ad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCoeff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1677\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 909\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    460\u001b[0m     return self._model_iteration(\n\u001b[0;32m    461\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m         steps=steps, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    442\u001b[0m               \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m               total_epochs=1)\n\u001b[0m\u001b[0;32m    445\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    492\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  Incompatible shapes: [94,2] vs. [100,2]\n\t [[node vae_mlp/encoder/z/mul (defined at C:\\Users\\sebas\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]] [Op:__inference_distributed_function_5663]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "x_test_decoded_vae = vae_ad.predict(Coeff[1677:,:1024], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-171-396cdd3c5a84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwasserstein\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mytrue\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mypred\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytrue\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mypred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "def wasserstein ( ytrue , ypred ):\n",
    "    return K.mean(ytrue * ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerG = RMSprop( lr =0.00005)\n",
    "optimizerD = RMSprop ( lr =0.00005)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
